{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### by Anastasiia Khaburska\n",
    "\n",
    "## Homework 5\n",
    "\n",
    "module : **Deep Learning for NLP**\n",
    "\n",
    "\n",
    "The goal of the homework is to develop a tool for Named Entity Recognition. You need to implement model **”Glove word embeddings + BiLSTM + Softmax”** for sequence labeling. Please, use the standard PyTorch example for the sequence labeling task ”Sequence models and long-short term memory networks”(https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#sphx-glr-beginner-nlp-sequence-models-tutorial-py) as a basic code to start. Glove word embeddings can be downloaded here (http://neuroner.com/data/word_vectors/glove.6B.100d.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import gensim\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 1**: \n",
    "Implement functionality to read and process NER 2003 English Shared Task data in CoNNL file format, data will be provided (10% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "    with open(f'data/{filename}.txt') as file:\n",
    "        size = 0\n",
    "        for line in file:\n",
    "            if line != '\\n':\n",
    "                w, _, _, _ = line.split(' ')\n",
    "                if (w!='-DOCSTART-'):\n",
    "                    size+=1\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "            columns=['sentence', 'word', 'POS_tag', 'SCHUNK_tag', 'NE_tag'],\n",
    "            data=np.zeros((size, 5))\n",
    "        )\n",
    "    df[:] = ''\n",
    "    with open(f'data/{filename}.txt') as file:\n",
    "        index = 0\n",
    "        word = 0\n",
    "        for line in file:\n",
    "            if line == '\\n':\n",
    "                index += 1\n",
    "            else:\n",
    "                w, p, s, n = line.split(' ')\n",
    "                if (w!='-DOCSTART-'):\n",
    "                    df.at[word, 'sentence'] = int(index)\n",
    "                    df.at[word, 'word'] = w.strip()\n",
    "                    df.at[word, 'POS_tag'] = p.strip()\n",
    "                    df.at[word, 'SCHUNK_tag'] = s.strip()               \n",
    "                    df.at[word, 'NE_tag'] = n.strip()\n",
    "                    word += 1\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process('train')\n",
    "dev = process('dev')\n",
    "test = process('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train: 203621\n",
      "Length of  dev:   51362\n",
      "Length of  test:  46435\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train: {len(train)}\")\n",
    "print(f\"Length of  dev:   {len(dev)}\")\n",
    "print(f\"Length of  test:  {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train: 14041\n",
      "Number of sentences in dev:   3250\n",
      "Number of sentences in test:  3453\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of sentences in train: {len(train['sentence'].unique())}\")\n",
    "print(f\"Number of sentences in dev:   {len(dev['sentence'].unique())}\")\n",
    "print(f\"Number of sentences in test:  {len(test['sentence'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entity tags: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Named entity tags:\",sorted(train['NE_tag'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    \n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {'B-LOC':0, 'B-MISC':1, 'B-ORG':2, 'B-PER':3, 'I-LOC':4, 'I-MISC':5, 'I-ORG':6, 'I-PER':7, 'O':8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "\n",
    "for word in train['word']:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "for word in dev['word']:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "for word in test['word']:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30289\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2**: \n",
    "\n",
    "Implement 3 strategies for loading the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings= {}\n",
    "\n",
    "with open('data/glove.6B.100d.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.split(' ')\n",
    "        word = elements[0]\n",
    "        word_embedding = np.array([float(val) for val in elements[1:]])\n",
    "        glove_embeddings[word] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector length:  100\n"
     ]
    }
   ],
   "source": [
    "embedding_length=len(glove_embeddings['the'])\n",
    "print(\"Embedding vector length: \", embedding_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a):** load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, associate it with UNKNOWN embedding (5% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix_a = np.zeros((vocab_size, 100))\n",
    "unknown_a=0\n",
    "for word, ix in word_to_ix.items():\n",
    "    try:\n",
    "        embeddings_matrix_a[ix, :] = glove_embeddings[word]\n",
    "    except KeyError as e:\n",
    "        embeddings_matrix_a[ix, :] = glove_embeddings['unknown']\n",
    "        unknown_a+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown:  15671\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unknown: \", unknown_a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b).** load the embeddings for lowercased capitalization of words. If embedding for this lowercased word doesn’t exists, associate it with UNKNOWN embedding (5% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix_b = np.zeros((vocab_size, 100))\n",
    "unknown_b=0\n",
    "for word, ix in word_to_ix.items():\n",
    "    try:\n",
    "        embeddings_matrix_b[ix, :] = glove_embeddings[word.lower()]\n",
    "    except KeyError as e:\n",
    "        embeddings_matrix_b[ix, :] = glove_embeddings['unknown']\n",
    "        unknown_b+=1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown:  3949\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unknown: \", unknown_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c).** load the embeddings for original capitalization of words. If embedding for this word doesn’t exists, try to find the embedding for lowercased version and associate it to the word with original capitalization. Otherwise, associate it with UNKNOWN embedding(20% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix_c = np.zeros((vocab_size, 100))\n",
    "unknown_c=0\n",
    "for word, ix in word_to_ix.items():\n",
    "    if word in glove_embeddings:\n",
    "        embeddings_matrix_c[ix, :] = glove_embeddings[word]\n",
    "    elif word.lower() in glove_embeddings:\n",
    "        embeddings_matrix_c[ix, :] = glove_embeddings[word.lower()]\n",
    "    else:\n",
    "        embeddings_matrix_c[ix, :] = glove_embeddings['unknown']\n",
    "        unknown_c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown:  3949\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unknown: \", unknown_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3**: \n",
    "\n",
    "Implement training on batches (20% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_length\n",
    "HIDDEN_DIM = 50\n",
    "EMBEDDING_MATRIX=embeddings_matrix_c\n",
    "VOCAB_SIZE=EMBEDDING_MATRIX.shape[0]\n",
    "TARGET_SIZE=len(tag_to_ix)\n",
    "DROPOUT=0.2\n",
    "LSTM_LAYER=2\n",
    "BATCH_SIZE=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "train_grouped = train.groupby(['sentence']).agg(lambda x: list(x)).reset_index(drop=True)\n",
    "for i in range(len(train_grouped)):\n",
    "    training_data.append((train_grouped.loc[i, 'word'], train_grouped.loc[i, 'NE_tag']))\n",
    "dev_grouped = dev.groupby(['sentence']).agg(lambda x: list(x)).reset_index(drop=True)\n",
    "for i in range(len(dev_grouped)):\n",
    "    dev_data.append((dev_grouped.loc[i, 'word'], dev_grouped.loc[i, 'NE_tag']))\n",
    "test_grouped = test.groupby(['sentence']).agg(lambda x: list(x)).reset_index(drop=True)\n",
    "for i in range(len(test_grouped)):\n",
    "    test_data.append((test_grouped.loc[i, 'word'], test_grouped.loc[i, 'NE_tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).from_pretrained(\n",
    "        torch.tensor(EMBEDDING_MATRIX, dtype=torch.float))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..............:\n",
      "Epoch 0 :___________________________________\n",
      "Micro-average of precision on dev :  0.9543242085588567\n",
      "Micro-average of recall on dev :  0.9543242085588567\n",
      "F1 score on dev :  0.9543242085588567\n",
      "F0.5 score on dev  :  0.9543242085588568\n",
      "Micro-average of precision on test :  0.9468719715731668\n",
      "Micro-average of recall on test :  0.9468719715731668\n",
      "F1 score on test :  0.9468719715731668\n",
      "F0.5 score on test  :  0.9468719715731668\n",
      "____________________________________________\n",
      "Epoch 1 :___________________________________\n",
      "Micro-average of precision on dev :  0.9616058564697636\n",
      "Micro-average of recall on dev :  0.9616058564697636\n",
      "F1 score on dev :  0.9616058564697636\n",
      "F0.5 score on dev  :  0.9616058564697635\n",
      "Micro-average of precision on test :  0.954797028103801\n",
      "Micro-average of recall on test :  0.954797028103801\n",
      "F1 score on test :  0.954797028103801\n",
      "F0.5 score on test  :  0.9547970281038011\n",
      "____________________________________________\n",
      "Epoch 2 :___________________________________\n",
      "Micro-average of precision on dev :  0.9660254662980413\n",
      "Micro-average of recall on dev :  0.9660254662980413\n",
      "F1 score on dev :  0.9660254662980413\n",
      "F0.5 score on dev  :  0.9660254662980413\n",
      "Micro-average of precision on test :  0.9585657370517928\n",
      "Micro-average of recall on test :  0.9585657370517928\n",
      "F1 score on test :  0.9585657370517928\n",
      "F0.5 score on test  :  0.9585657370517928\n",
      "____________________________________________\n",
      "Epoch 3 :___________________________________\n",
      "Micro-average of precision on dev :  0.9680113702737433\n",
      "Micro-average of recall on dev :  0.9680113702737433\n",
      "F1 score on dev :  0.9680113702737433\n",
      "F0.5 score on dev  :  0.9680113702737433\n",
      "Micro-average of precision on test :  0.9601808980295036\n",
      "Micro-average of recall on test :  0.9601808980295036\n",
      "F1 score on test :  0.9601808980295036\n",
      "F0.5 score on test  :  0.9601808980295035\n",
      "____________________________________________\n",
      "Epoch 4 :___________________________________\n",
      "Micro-average of precision on dev :  0.9691406097893385\n",
      "Micro-average of recall on dev :  0.9691406097893385\n",
      "F1 score on dev :  0.9691406097893385\n",
      "F0.5 score on dev  :  0.9691406097893385\n",
      "Micro-average of precision on test :  0.9607192850220738\n",
      "Micro-average of recall on test :  0.9607192850220738\n",
      "F1 score on test :  0.9607192850220738\n",
      "F0.5 score on test  :  0.9607192850220738\n",
      "____________________________________________\n",
      "Epoch 5 :___________________________________\n",
      "Micro-average of precision on dev :  0.9697441688407772\n",
      "Micro-average of recall on dev :  0.9697441688407772\n",
      "F1 score on dev :  0.9697441688407772\n",
      "F0.5 score on dev  :  0.9697441688407773\n",
      "Micro-average of precision on test :  0.961042317217616\n",
      "Micro-average of recall on test :  0.961042317217616\n",
      "F1 score on test :  0.961042317217616\n",
      "F0.5 score on test  :  0.961042317217616\n",
      "____________________________________________\n",
      "Epoch 6 :___________________________________\n",
      "Micro-average of precision on dev :  0.9697831081344185\n",
      "Micro-average of recall on dev :  0.9697831081344185\n",
      "F1 score on dev :  0.9697831081344185\n",
      "F0.5 score on dev  :  0.9697831081344184\n",
      "Micro-average of precision on test :  0.9610853881770216\n",
      "Micro-average of recall on test :  0.9610853881770216\n",
      "F1 score on test :  0.9610853881770216\n",
      "F0.5 score on test  :  0.9610853881770215\n",
      "____________________________________________\n",
      "Epoch 7 :___________________________________\n",
      "Micro-average of precision on dev :  0.9698804563685215\n",
      "Micro-average of recall on dev :  0.9698804563685215\n",
      "F1 score on dev :  0.9698804563685215\n",
      "F0.5 score on dev  :  0.9698804563685215\n",
      "Micro-average of precision on test :  0.96114999461613\n",
      "Micro-average of recall on test :  0.96114999461613\n",
      "F1 score on test :  0.96114999461613\n",
      "F0.5 score on test  :  0.9611499946161299\n",
      "____________________________________________\n",
      "Epoch 8 :___________________________________\n",
      "Micro-average of precision on dev :  0.9696857599003154\n",
      "Micro-average of recall on dev :  0.9696857599003154\n",
      "F1 score on dev :  0.9696857599003154\n",
      "F0.5 score on dev  :  0.9696857599003154\n",
      "Micro-average of precision on test :  0.9604177883062345\n",
      "Micro-average of recall on test :  0.9604177883062345\n",
      "F1 score on test :  0.9604177883062345\n",
      "F0.5 score on test  :  0.9604177883062345\n",
      "____________________________________________\n",
      "Epoch 9 :___________________________________\n",
      "Micro-average of precision on dev :  0.9699193956621627\n",
      "Micro-average of recall on dev :  0.9699193956621627\n",
      "F1 score on dev :  0.9699193956621627\n",
      "F0.5 score on dev  :  0.9699193956621627\n",
      "Micro-average of precision on test :  0.9605254657047486\n",
      "Micro-average of recall on test :  0.9605254657047486\n",
      "F1 score on test :  0.9605254657047486\n",
      "F0.5 score on test  :  0.9605254657047486\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMTagger(EMBEDDING_DIM , HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "losses=[]\n",
    "epochs=[]\n",
    "\n",
    "print(\"Start training..............:\")\n",
    "model.train()\n",
    "model.zero_grad()\n",
    "for epoch in range(10): \n",
    "    model.train()\n",
    "    order=list(range(len(training_data)))\n",
    "    for i in range((len(training_data)+1)//BATCH_SIZE):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        batch_sentences=[prepare_sequence(training_data[idx][0], \n",
    "                                          word_to_ix) for idx in order[i*BATCH_SIZE:(i+1)*BATCH_SIZE]]\n",
    "        batch_tags=[prepare_sequence(training_data[idx][1], \n",
    "                                          tag_to_ix) for idx in order[i*BATCH_SIZE:(i+1)*BATCH_SIZE]]\n",
    "        for sentence_in, targets in zip(batch_sentences,batch_tags):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance        \n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    epochs.append(epoch)\n",
    "    print(f'Epoch {epoch} :___________________________________')\n",
    "    results_matrix_=results_matr(model, dev_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on dev : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on dev : ', micro_aver_recall_)\n",
    "    print('F1 score on dev : ', f1_)\n",
    "    print('F0.5 score on dev  : ', f05_)\n",
    "    results_matrix_=results_matr(model, test_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on test : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on test : ', micro_aver_recall_)\n",
    "    print('F1 score on test : ', f1_)\n",
    "    print('F0.5 score on test  : ', f05_)\n",
    "    print('____________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/bimodel10.pkl'\n",
    "output = open(filename, 'wb')\n",
    "pickle.dump(model,output)\n",
    "with open(filename, 'rb') as pickle_file:\n",
    "    model_loaded = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (word_embeddings): Embedding(30289, 100)\n",
       "  (lstm): LSTM(100, 50, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=100, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (word_embeddings): Embedding(30289, 100)\n",
       "  (lstm): LSTM(100, 50, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=100, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 4**: \n",
    "\n",
    "Implement the calculation of token-level Precision / Recall / F1 /F 0.5 scores for all classes in average. IMPORTANT! Please, implement “micro-average”(https://tomaxent.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/) approach. Don’t use standard functions from scikit-learn or similar external packages (30% of score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n",
      "{'TP': 0, 'TN': 1, 'FP': 2, 'FN': 3}\n",
      "9\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "result_types={'TP':0, 'TN':1, 'FP':2, 'FN':3}\n",
    "print(tag_to_ix)\n",
    "print(result_types)\n",
    "print(len(tag_to_ix))\n",
    "print(len(result_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_matr(resmodel, testdata):\n",
    "    results_matrix=np.zeros((len(tag_to_ix), len(result_types)))\n",
    "    with torch.no_grad():\n",
    "        for w_t in testdata:\n",
    "            inputs = prepare_sequence(w_t[0], word_to_ix)\n",
    "            scores = resmodel(inputs)\n",
    "            outputs = scores.numpy().argmax(axis=1)\n",
    "            tags=prepare_sequence(w_t[1], tag_to_ix)\n",
    "            for i in range(len(outputs)):\n",
    "                pr=outputs[i]\n",
    "                tr=tags[i]\n",
    "                if pr==tr:\n",
    "                    results_matrix[tr,0]+=1\n",
    "                    results_matrix[:,1]+=1\n",
    "                    results_matrix[tr,1]-=1\n",
    "                else:\n",
    "                    results_matrix[pr,2]+=1\n",
    "                    results_matrix[tr,3]+=1\n",
    "                    results_matrix[:,1]+=1\n",
    "                    results_matrix[pr,1]-=1\n",
    "                    results_matrix[tr,1]-=1\n",
    "    return results_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_matrix=results_matr(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6590e+03, 4.9426e+04, 9.9000e+01, 1.7800e+02],\n",
       "       [7.3300e+02, 5.0309e+04, 1.3100e+02, 1.8900e+02],\n",
       "       [1.1230e+03, 4.9824e+04, 1.9700e+02, 2.1800e+02],\n",
       "       [1.7390e+03, 4.9406e+04, 1.1400e+02, 1.0300e+02],\n",
       "       [2.0300e+02, 5.1062e+04, 4.3000e+01, 5.4000e+01],\n",
       "       [1.9400e+02, 5.0942e+04, 7.4000e+01, 1.5200e+02],\n",
       "       [5.1900e+02, 5.0478e+04, 1.3300e+02, 2.3200e+02],\n",
       "       [1.2280e+03, 5.0007e+04, 4.8000e+01, 7.9000e+01],\n",
       "       [4.2419e+04, 7.8970e+03, 7.0600e+02, 3.4000e+02]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-average Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_aver_precision = sum(results_matrix[:,0])/(sum(results_matrix[:,0])+sum(results_matrix[:,2]))\n",
    "micro_aver_recall = sum(results_matrix[:,0])/(sum(results_matrix[:,0])+sum(results_matrix[:,3]))\n",
    "f1=2*micro_aver_precision*micro_aver_recall/(micro_aver_precision+micro_aver_recall)\n",
    "f05=1.25*micro_aver_precision*micro_aver_recall/((0.25*micro_aver_precision)+micro_aver_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average of precision :  0.9699193956621627\n",
      "Micro-average of recall :  0.9699193956621627\n",
      "F1 score :  0.9699193956621627\n",
      "F0.5 score :  0.9699193956621627\n"
     ]
    }
   ],
   "source": [
    "print('Micro-average of precision : ', micro_aver_precision)\n",
    "print('Micro-average of recall : ', micro_aver_recall)\n",
    "print('F1 score : ', f1)\n",
    "print('F0.5 score : ', f05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If class A is predicted and the true label is B, then there is a FP for A and a FN for B. If the prediction is correct, i.e. class A is predicted and A is also the true label, denn there is neither a false positive nor a false negative but only a true positive. So there is no possibility that would increase only FP or FN but not both. That is why precision and recall are always the same when using the micro averaging scheme.* - https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_aver_precision_only_for_classes = sum(results_matrix[:8,0])/(sum(results_matrix[:8,0])+sum(results_matrix[:8,2]))\n",
    "micro_aver_recall_only_for_classes = sum(results_matrix[:8,0])/(sum(results_matrix[:8,0])+sum(results_matrix[:8,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-average of precision only for NE tags :  0.8981425276192788\n",
      "Micro-average of recall only for NE tags :  0.8599325816575614\n"
     ]
    }
   ],
   "source": [
    "print('Micro-average of precision only for NE tags : ', micro_aver_precision_only_for_classes)\n",
    "print('Micro-average of recall only for NE tags : ', micro_aver_recall_only_for_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Besides micro averaging, one might also consider weighted averaging in case of an unequally distributed data set.*\n",
    "\n",
    "**Macro-average Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94368601 0.84837963 0.85075758 0.93847814 0.82520325 0.7238806\n",
      " 0.79601227 0.96238245 0.98362899]\n",
      "[0.90310289 0.79501085 0.83743475 0.94408252 0.78988327 0.56069364\n",
      " 0.69107856 0.93955624 0.99204846]\n"
     ]
    }
   ],
   "source": [
    "precisions=results_matrix[:,0]/(results_matrix[:,0]+results_matrix[:,2])\n",
    "recalls=results_matrix[:,0]/(results_matrix[:,0]+results_matrix[:,3])\n",
    "print(precisions)\n",
    "print(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_aver_precision = sum(precisions)/9\n",
    "macro_aver_recall = sum(recalls)/9\n",
    "macro_f1=2*macro_aver_precision*macro_aver_recall/(macro_aver_precision+macro_aver_recall)\n",
    "macro_f05=1.25*macro_aver_precision*macro_aver_recall/((0.25*macro_aver_precision)+macro_aver_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-average of precision :  0.874712100599839\n",
      "Macro-average of recall :  0.8280990184022845\n",
      "Macro F1 score :  0.8507675617197139\n",
      "Macro F0.5 score :  0.8649743471556832\n"
     ]
    }
   ],
   "source": [
    "print('Macro-average of precision : ', macro_aver_precision)\n",
    "print('Macro-average of recall : ', macro_aver_recall)\n",
    "print('Macro F1 score : ', macro_f1)\n",
    "print('Macro F0.5 score : ',macro_f05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 5**: \n",
    "\n",
    "5. Provide the report the performances (F1 and F0.5 scores) on the dev/ test subsets w.r.t epoch number during the training for the first 5 epochs for each strategy of loading the embeddings (10% of score).\n",
    "\n",
    "**strategy a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_length\n",
    "HIDDEN_DIM = 50\n",
    "EMBEDDING_MATRIX=embeddings_matrix_a\n",
    "VOCAB_SIZE=EMBEDDING_MATRIX.shape[0]\n",
    "TARGET_SIZE=len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger_M(nn.Module):\n",
    "\n",
    "    def __init__(self,EMBEDDING_MATRIX,embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BiLSTMTagger_M, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim).from_pretrained(\n",
    "        torch.tensor(EMBEDDING_MATRIX, dtype=torch.float))\n",
    "        self.word_embeddings.weight.requires_grad = False\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training strategy a..............:\n",
      "Epoch 0 :___________________________________\n",
      "Micro-average of precision on dev :  0.9024375997819399\n",
      "Micro-average of recall on dev :  0.9024375997819399\n",
      "F1 score on dev :  0.9024375997819399\n",
      "F0.5 score on dev  :  0.9024375997819399\n",
      "Micro-average of precision on test :  0.8970819425002692\n",
      "Micro-average of recall on test :  0.8970819425002692\n",
      "F1 score on test :  0.8970819425002692\n",
      "F0.5 score on test  :  0.8970819425002693\n",
      "____________________________________________\n",
      "Epoch 1 :___________________________________\n",
      "Micro-average of precision on dev :  0.909466142284179\n",
      "Micro-average of recall on dev :  0.909466142284179\n",
      "F1 score on dev :  0.909466142284179\n",
      "F0.5 score on dev  :  0.909466142284179\n",
      "Micro-average of precision on test :  0.9013675029611284\n",
      "Micro-average of recall on test :  0.9013675029611284\n",
      "F1 score on test :  0.9013675029611284\n",
      "F0.5 score on test  :  0.9013675029611284\n",
      "____________________________________________\n",
      "Epoch 2 :___________________________________\n",
      "Micro-average of precision on dev :  0.9126591643627585\n",
      "Micro-average of recall on dev :  0.9126591643627585\n",
      "F1 score on dev :  0.9126591643627585\n",
      "F0.5 score on dev  :  0.9126591643627584\n",
      "Micro-average of precision on test :  0.904834715193281\n",
      "Micro-average of recall on test :  0.904834715193281\n",
      "F1 score on test :  0.904834715193281\n",
      "F0.5 score on test  :  0.9048347151932811\n",
      "____________________________________________\n",
      "Epoch 3 :___________________________________\n",
      "Micro-average of precision on dev :  0.9141777968147657\n",
      "Micro-average of recall on dev :  0.9141777968147657\n",
      "F1 score on dev :  0.9141777968147657\n",
      "F0.5 score on dev  :  0.9141777968147657\n",
      "Micro-average of precision on test :  0.906988263163562\n",
      "Micro-average of recall on test :  0.906988263163562\n",
      "F1 score on test :  0.906988263163562\n",
      "F0.5 score on test  :  0.906988263163562\n",
      "____________________________________________\n",
      "Epoch 4 :___________________________________\n",
      "Micro-average of precision on dev :  0.9166309723141622\n",
      "Micro-average of recall on dev :  0.9166309723141622\n",
      "F1 score on dev :  0.9166309723141622\n",
      "F0.5 score on dev  :  0.9166309723141622\n",
      "Micro-average of precision on test :  0.9089910627759233\n",
      "Micro-average of recall on test :  0.9089910627759233\n",
      "F1 score on test :  0.9089910627759233\n",
      "F0.5 score on test  :  0.9089910627759233\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_a = BiLSTMTagger_M(EMBEDDING_MATRIX,EMBEDDING_DIM , HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_a.parameters(), lr=0.1)\n",
    "\n",
    "print(\"Start training strategy a..............:\")\n",
    "for epoch in range(5): \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance        \n",
    "        model_a.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model_a(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} :___________________________________')\n",
    "    results_matrix_=results_matr(model_a, dev_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on dev : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on dev : ', micro_aver_recall_)\n",
    "    print('F1 score on dev : ', f1_)\n",
    "    print('F0.5 score on dev  : ', f05_)\n",
    "    results_matrix_=results_matr(model_a, test_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on test : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on test : ', micro_aver_recall_)\n",
    "    print('F1 score on test : ', f1_)\n",
    "    print('F0.5 score on test  : ', f05_)\n",
    "    print('____________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**strategy b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_length\n",
    "HIDDEN_DIM = 50\n",
    "EMBEDDING_MATRIX=embeddings_matrix_b\n",
    "VOCAB_SIZE=EMBEDDING_MATRIX.shape[0]\n",
    "TARGET_SIZE=len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training strategy b..............:\n",
      "Epoch 0 :___________________________________\n",
      "Micro-average of precision on dev :  0.9503134613138118\n",
      "Micro-average of recall on dev :  0.9503134613138118\n",
      "F1 score on dev :  0.9503134613138118\n",
      "F0.5 score on dev  :  0.9503134613138118\n",
      "Micro-average of precision on test :  0.9452998815548617\n",
      "Micro-average of recall on test :  0.9452998815548617\n",
      "F1 score on test :  0.9452998815548617\n",
      "F0.5 score on test  :  0.9452998815548616\n",
      "____________________________________________\n",
      "Epoch 1 :___________________________________\n",
      "Micro-average of precision on dev :  0.9599119971963709\n",
      "Micro-average of recall on dev :  0.9599119971963709\n",
      "F1 score on dev :  0.9599119971963709\n",
      "F0.5 score on dev  :  0.9599119971963709\n",
      "Micro-average of precision on test :  0.9540432863142027\n",
      "Micro-average of recall on test :  0.9540432863142027\n",
      "F1 score on test :  0.9540432863142027\n",
      "F0.5 score on test  :  0.9540432863142025\n",
      "____________________________________________\n",
      "Epoch 2 :___________________________________\n",
      "Micro-average of precision on dev :  0.9643900159651104\n",
      "Micro-average of recall on dev :  0.9643900159651104\n",
      "F1 score on dev :  0.9643900159651104\n",
      "F0.5 score on dev  :  0.9643900159651105\n",
      "Micro-average of precision on test :  0.9574674275869495\n",
      "Micro-average of recall on test :  0.9574674275869495\n",
      "F1 score on test :  0.9574674275869495\n",
      "F0.5 score on test  :  0.9574674275869494\n",
      "____________________________________________\n",
      "Epoch 3 :___________________________________\n",
      "Micro-average of precision on dev :  0.9668431914645068\n",
      "Micro-average of recall on dev :  0.9668431914645068\n",
      "F1 score on dev :  0.9668431914645068\n",
      "F0.5 score on dev  :  0.9668431914645069\n",
      "Micro-average of precision on test :  0.958781091848821\n",
      "Micro-average of recall on test :  0.958781091848821\n",
      "F1 score on test :  0.958781091848821\n",
      "F0.5 score on test  :  0.958781091848821\n",
      "____________________________________________\n",
      "Epoch 4 :___________________________________\n",
      "Micro-average of precision on dev :  0.9681476578014875\n",
      "Micro-average of recall on dev :  0.9681476578014875\n",
      "F1 score on dev :  0.9681476578014875\n",
      "F0.5 score on dev  :  0.9681476578014875\n",
      "Micro-average of precision on test :  0.959793259394853\n",
      "Micro-average of recall on test :  0.959793259394853\n",
      "F1 score on test :  0.9597932593948532\n",
      "F0.5 score on test  :  0.959793259394853\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b = BiLSTMTagger_M(EMBEDDING_MATRIX,EMBEDDING_DIM , HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_b.parameters(), lr=0.1)\n",
    "\n",
    "print(\"Start training strategy b..............:\")\n",
    "for epoch in range(5): \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance        \n",
    "        model_b.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model_b(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} :___________________________________')\n",
    "    results_matrix_=results_matr(model_b, dev_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on dev : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on dev : ', micro_aver_recall_)\n",
    "    print('F1 score on dev : ', f1_)\n",
    "    print('F0.5 score on dev  : ', f05_)\n",
    "    results_matrix_=results_matr(model_b, test_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on test : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on test : ', micro_aver_recall_)\n",
    "    print('F1 score on test : ', f1_)\n",
    "    print('F0.5 score on test  : ', f05_)\n",
    "    print('____________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**strategy c**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = embedding_length\n",
    "HIDDEN_DIM = 50\n",
    "EMBEDDING_MATRIX=embeddings_matrix_c\n",
    "VOCAB_SIZE=EMBEDDING_MATRIX.shape[0]\n",
    "TARGET_SIZE=len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training strategy c..............:\n",
      "Epoch 0 :___________________________________\n",
      "Micro-average of precision on dev :  0.9521436081149488\n",
      "Micro-average of recall on dev :  0.9521436081149488\n",
      "F1 score on dev :  0.9521436081149488\n",
      "F0.5 score on dev  :  0.9521436081149489\n",
      "Micro-average of precision on test :  0.9457736621083235\n",
      "Micro-average of recall on test :  0.9457736621083235\n",
      "F1 score on test :  0.9457736621083235\n",
      "F0.5 score on test  :  0.9457736621083235\n",
      "____________________________________________\n",
      "Epoch 1 :___________________________________\n",
      "Micro-average of precision on dev :  0.9602040418986799\n",
      "Micro-average of recall on dev :  0.9602040418986799\n",
      "F1 score on dev :  0.9602040418986799\n",
      "F0.5 score on dev  :  0.9602040418986798\n",
      "Micro-average of precision on test :  0.9540863572736082\n",
      "Micro-average of recall on test :  0.9540863572736082\n",
      "F1 score on test :  0.9540863572736082\n",
      "F0.5 score on test  :  0.9540863572736082\n",
      "____________________________________________\n",
      "Epoch 2 :___________________________________\n",
      "Micro-average of precision on dev :  0.9658891787702971\n",
      "Micro-average of recall on dev :  0.9658891787702971\n",
      "F1 score on dev :  0.9658891787702971\n",
      "F0.5 score on dev  :  0.9658891787702971\n",
      "Micro-average of precision on test :  0.9587164854097125\n",
      "Micro-average of recall on test :  0.9587164854097125\n",
      "F1 score on test :  0.9587164854097125\n",
      "F0.5 score on test  :  0.9587164854097125\n",
      "____________________________________________\n",
      "Epoch 3 :___________________________________\n",
      "Micro-average of precision on dev :  0.968069779214205\n",
      "Micro-average of recall on dev :  0.968069779214205\n",
      "F1 score on dev :  0.968069779214205\n",
      "F0.5 score on dev  :  0.968069779214205\n",
      "Micro-average of precision on test :  0.9599009367933671\n",
      "Micro-average of recall on test :  0.9599009367933671\n",
      "F1 score on test :  0.9599009367933671\n",
      "F0.5 score on test  :  0.959900936793367\n",
      "____________________________________________\n",
      "Epoch 4 :___________________________________\n",
      "Micro-average of precision on dev :  0.9692184883766208\n",
      "Micro-average of recall on dev :  0.9692184883766208\n",
      "F1 score on dev :  0.9692184883766208\n",
      "F0.5 score on dev  :  0.9692184883766207\n",
      "Micro-average of precision on test :  0.9609992462582104\n",
      "Micro-average of recall on test :  0.9609992462582104\n",
      "F1 score on test :  0.9609992462582104\n",
      "F0.5 score on test  :  0.9609992462582105\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_c = BiLSTMTagger_M(EMBEDDING_MATRIX,EMBEDDING_DIM , HIDDEN_DIM, VOCAB_SIZE, TARGET_SIZE)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model_c.parameters(), lr=0.1)\n",
    "\n",
    "print(\"Start training strategy c..............:\")\n",
    "for epoch in range(5): \n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance        \n",
    "        model_c.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        \n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model_c(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch} :___________________________________')\n",
    "    results_matrix_=results_matr(model_c, dev_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on dev : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on dev : ', micro_aver_recall_)\n",
    "    print('F1 score on dev : ', f1_)\n",
    "    print('F0.5 score on dev  : ', f05_)\n",
    "    results_matrix_=results_matr(model_c, test_data)\n",
    "    micro_aver_precision_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,2]))\n",
    "    micro_aver_recall_ = sum(results_matrix_[:,0])/(sum(results_matrix_[:,0])+sum(results_matrix_[:,3]))\n",
    "    f1_=2*micro_aver_precision_*micro_aver_recall_/(micro_aver_precision_+micro_aver_recall_)\n",
    "    f05_=1.25*micro_aver_precision_*micro_aver_recall_/((0.25*micro_aver_precision_)+micro_aver_recall_)\n",
    "    print('Micro-average of precision on test : ', micro_aver_precision_)\n",
    "    print('Micro-average of recall on test : ', micro_aver_recall_)\n",
    "    print('F1 score on test : ', f1_)\n",
    "    print('F0.5 score on test  : ', f05_)\n",
    "    print('____________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger_M(\n",
       "  (word_embeddings): Embedding(30289, 100)\n",
       "  (lstm): LSTM(100, 50, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=100, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/bimodel_c5.pkl'\n",
    "output = open(filename, 'wb')\n",
    "pickle.dump(model_c,output)\n",
    "with open(filename, 'rb') as pickle_file:\n",
    "    model_c_loaded = pickle.load(pickle_file)\n",
    "filename = 'data/bimodel_a5.pkl'\n",
    "output = open(filename, 'wb')\n",
    "pickle.dump(model_a,output)\n",
    "with open(filename, 'rb') as pickle_file:\n",
    "    model_a_loaded = pickle.load(pickle_file)\n",
    "filename = 'data/bimodel_b5.pkl'\n",
    "output = open(filename, 'wb')\n",
    "pickle.dump(model_b,output)\n",
    "with open(filename, 'rb') as pickle_file:\n",
    "    model_b_loaded = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
